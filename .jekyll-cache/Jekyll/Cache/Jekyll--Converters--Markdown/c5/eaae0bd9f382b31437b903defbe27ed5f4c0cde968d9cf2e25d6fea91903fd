I"m<p>지난주, Coursera Andrew Ng 교수님의 Deep learning specialization을 마무리했습니다. 이번주부터는 굉장히 섬세한 fake 이미지를 생성할 수 있는 GAN 모델을 다루는 강의를 시청하고 리뷰할 예정입니다.</p>

<p>GAN 이란 Generative Adverarial Network의 약자로, 하나의 모델이 가짜 이미지를 생성하고 다른 모델이 진짜 이미지와 가짜 이미지를 구분? 하면서 Generator 모델이 더욱 완벽한 fake 이미지를 만들 수 있도록 training 합니다.</p>

<h2 id="generative-models">Generative Models</h2>

<p><img src="/img/AI_study/gan1.png" alt="GAN" style="zoom:50%;" /></p>

<p><strong>Generator</strong></p>

<ul>
  <li>Random noise를 input에 주고 fake image를 생성합니다.</li>
</ul>

<p><strong>Discriminator</strong></p>

<ul>
  <li>Real image를 input에 주고 학습시킵니다.</li>
</ul>

<p>학습이 완료되면 Random Noise를 input으로 Generator에 주게 되면 정교한 fake 이미지를 생성하게 됩니다. 그리고 위 모델들이 경쟁하며 성능을 향상시키는 구조를 갖고 있습니다.</p>

<h4 id="summary">Summary</h4>

<ul>
  <li>Generative models learn to produce <strong>realistic</strong> examples</li>
  <li>Discriminative models distinguish between classes</li>
</ul>

<h2 id="real-life-gans">Real Life GANs</h2>

<p>Ian Goodfellow is known as the creator of the GAN.</p>

<p>not limited to human faces</p>

<p>image translation : horse to zebra, vice versa</p>

<p>draw something -&gt; able to produce realistic image</p>

<p>Monariza -&gt; moving</p>

<p>Companies Using GANs</p>

<ul>
  <li>Adobe</li>
  <li>Google (text generation)</li>
  <li>IBM (data augmentation) : x enough data -&gt; augmentation</li>
  <li>whatever you like</li>
</ul>

<h4 id="summary-1">Summary</h4>

<ul>
  <li>GANs’ performance is rapidly improving</li>
  <li>Huge opportunity to work in this space!</li>
  <li>Major companies are using them</li>
</ul>

<h2 id="intuition-behind-gans">Intuition Behind GANs</h2>

<h4 id="outline">Outline</h4>

<ul>
  <li>
    <p>the goal of the generator and the discriminator</p>
  </li>
  <li>
    <p>the competition between them</p>
  </li>
</ul>

<p>Generator : learn to make fakes that look real</p>

<p>Discriminator : learns to distinguish real from fake</p>

<p>First, train a discriminator</p>

<h4 id="summary-2">Summary</h4>

<ul>
  <li>The <strong>generator’s</strong> goal is to fool the discriminator</li>
  <li>The <strong>discriminator’s</strong> goal is to distinguish between real and fake</li>
  <li>They learn from the competition with each other</li>
  <li>At the end, <strong>fakes</strong> look <strong>real</strong></li>
</ul>

<h2 id="discriminator">Discriminator</h2>

<h4 id="outline-1">Outline</h4>

<ul>
  <li>Review of classifiers</li>
  <li>The role of classifiers in terms of probability</li>
  <li>Discriminator</li>
</ul>

<p>Discriminator : Distinguish between different classes</p>

<h4 id="summary-3">Summary</h4>

<ul>
  <li>The <strong>discriminator</strong> is a classifier</li>
  <li>It learns the probability of class Y (<strong>real</strong> or <strong>fake</strong>) given features X</li>
  <li>The probabilities are the feedback for the <strong>generator</strong></li>
</ul>

<h2 id="generator">Generator</h2>

<h4 id="outline-2">Outline</h4>

<ul>
  <li>What the generator does</li>
  <li>How it improves its performance</li>
  <li>Generator in terms of probability</li>
</ul>

<p>Noise vector - different output</p>

<p>Noise -&gt; <strong>Generator</strong> -&gt; Features(X hat) -&gt; **Discriminator ** -&gt; Output (Y hat) -&gt; Cost -&gt; update Generator’s Parameters</p>

<h4 id="summary-4">Summary</h4>

<ul>
  <li>The <strong>Generator</strong> produces fake data</li>
  <li>It learns the probability of features X</li>
  <li>The <strong>Generator</strong> takes as input noise (random features)</li>
</ul>

<h2 id="bce-cost-function">BCE Cost Function</h2>

<h4 id="outline-3">Outline</h4>

<ul>
  <li>Binary Cross Entropy(BCE) Loss equation by parts</li>
  <li>How it looks graphically</li>
</ul>

<h4 id="summary-5">Summary</h4>

<ul>
  <li>The BCE cost function has two parts (one relevant for each class)</li>
  <li>Close to zero when label and the prediction are similar</li>
  <li>Approaches infinity when the label and the prediction are different</li>
</ul>

<h2 id="putting-it-all-together">Putting It All Together</h2>

<h4 id="outline-4">Outline</h4>

<ul>
  <li>How the whole architecture looks</li>
  <li>How to train GANs</li>
</ul>

<h4 id="summary-6">Summary</h4>

<ul>
  <li>GANs train in an alternating fashion</li>
  <li>The two models should always be at a similar “skill” level</li>
</ul>

<h2 id="intro-to-pytorch">Intro to PyTorch</h2>

<h4 id="outline-5">Outline</h4>

<ul>
  <li>Comparison with TensorFlow</li>
  <li>Dfining Models</li>
  <li>Training</li>
</ul>

<p><img src="img/AI_study/pytorch_vs_tensorflow.png" alt="screenshot" /></p>

<h4 id="summary-7">Summary</h4>

<ul>
  <li>PyTorch makes computations on the run</li>
  <li>Dynamic computational graphs in PyTorch</li>
  <li><strong>Just another framework, and similar to Tensorflow!</strong></li>
</ul>

:ET